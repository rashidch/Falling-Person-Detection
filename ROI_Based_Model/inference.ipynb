{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Import packages and modules from Python Standard Library and Third party libraries.\n",
    "'''\n",
    "#Import from python standard library\n",
    "import os\n",
    "\n",
    "#Import from third party libraries \n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from skimage.color import gray2rgb, rgb2gray\n",
    "from tensorflow import keras\n",
    "import timeit\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_boxes():\n",
    "    \n",
    "    '''\n",
    "        required directory hierarchy: current_directory/output/file.txt\n",
    "    '''\n",
    "    #Read files and get a dictionary with image 'name.png' as key and 'bounding box coordinates' as value\n",
    "    #Each file contains name.png and bounding box coordinates in each row\n",
    "    allBoxes = {}\n",
    "    files = ['FallingDown_01.txt','FallingDown_02.txt','Standing_01.txt','Standing_02.txt']\n",
    "    path = './output/'\n",
    "\n",
    "    for filename in files:\n",
    "        boxes = {}\n",
    "        with open(path+filename,'r') as file:\n",
    "            for line in file:\n",
    "                tokens = line.split(' ',1)\n",
    "                name,box = tokens[0].strip(), tokens[1].strip()\n",
    "                boxes[name] = box\n",
    "        key = filename.split('.')[0]\n",
    "        allBoxes[key]=boxes\n",
    "    \n",
    "    return allBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    '''\n",
    "     required directory hierarchy: current_directory/output/DIRs\n",
    "    '''\n",
    "    \n",
    "    allBoxes = read_boxes() \n",
    "    #define list to store the dataset \n",
    "    dataset = []\n",
    "    #define list to store the data labels \n",
    "    dataLabels = []\n",
    "    #list of main-directory names\n",
    "    DIRs = ['FallingDown_01','FallingDown_02','Standing_01','Standing_02']\n",
    "    #iterate over each main-directory name\n",
    "    for DIR in DIRs:\n",
    "        #join path with each main-directory name\n",
    "        path = './output/'+str(DIR)\n",
    "        print('DIR',DIR)\n",
    "        #Extract the path of all sub-directories inside each main-directory \n",
    "        for directory in glob.glob(path):\n",
    "            sub_path = os.path.join(directory+'/*.png')\n",
    "            #extract path of all images in sub-directory\n",
    "            image_paths = glob.glob(sub_path)\n",
    "            #split the directory path and extract directory name\n",
    "            labels = directory.split('\\\\')[-1]\n",
    "            #Iterate over each image path to read the image \n",
    "            for image_path in image_paths:\n",
    "                name = os.path.basename(image_path)\n",
    "                try:\n",
    "                    box = allBoxes[DIR][name]\n",
    "                    box_tokens = [ tk for tk in box.strip('[,]').split(' ') if tk is not '']\n",
    "                    x1 = box_tokens[0].strip(' ')\n",
    "                    y1 = box_tokens[1].strip(' ')\n",
    "                    x2 = box_tokens[2].strip(' ')\n",
    "                    y2 = box_tokens[3].strip(' ')\n",
    "                    box = x1,y1,x2,y2\n",
    "                    x1,y1,x2,y2 = int(x1),int(y1),int(x2),int(y2)\n",
    "                    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "                    image = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)\n",
    "                    #print(image.shape)\n",
    "                    image = image[y1:y2,x1:x2]\n",
    "                    image = cv2.resize(image,dsize=(300,300), interpolation= cv2.INTER_AREA)\n",
    "                    output_path = path+'/cropped/'+name+'.png'\n",
    "                    cv2.imwrite(output_path,cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "                    #print(image.shape)\n",
    "                    #convert to rgb if grayscale image \n",
    "                    if len(image.shape)==2:\n",
    "                        image = gray2rbg(image)\n",
    "                    if DIR is 'FallingDown_01' or DIR is 'FallingDown_02':\n",
    "                        #append image to dataset list  \n",
    "                        dataset.append(image)\n",
    "                        dataLabels.append(int(1))\n",
    "                    elif DIR=='Standing_01' or DIR=='Standing_02':\n",
    "                        dataset.append(image)\n",
    "                        dataLabels.append(int(0))\n",
    "                except KeyError:\n",
    "                    print('Key Error')\n",
    "\n",
    "    #convert dataset and data_labels to numpy array        \n",
    "    dataset = np.array(dataset)\n",
    "    dataLabels = np.array(dataLabels)\n",
    "    \n",
    "    return dataset, dataLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#methods to rescale images\n",
    "def rescaleImage(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255.0\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestimage(dataset, dataLabels):\n",
    "    '''\n",
    "        insert batch =1 in \"test_dataset.map(rescaleImage).shuffle(buffer_size=1024).batch(4)\"  to get one image\n",
    "    '''\n",
    "    \n",
    "    #shuffle dataset and split dataset into train, test and validation sets\n",
    "    dataset, dataLabels = shuffle(dataset, dataLabels, random_state=1236)\n",
    "\n",
    "    #train_x, train_y = dataset[:1000], dataLabels[:1000]\n",
    "    test_x, test_y   = dataset[1000:1400], dataLabels[1000:1400]\n",
    "    #valid_x, valid_y = dataset[1400:], dataLabels[1400:]\n",
    "    \n",
    "    #convert train, test and validation sets to tensorflow datasets \n",
    "    test_dataset   = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "    \n",
    "    #map test dataset to preprocessing function, shuffle, \n",
    "    test_dataset  = test_dataset.map(rescaleImage).shuffle(buffer_size=1024).batch(30)\n",
    "    image, label  = test_dataset.as_numpy_iterator().next()\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_detection_segmentation(image, model):\n",
    "    \n",
    "    \n",
    "    #inference on single image\n",
    "    prediction = model.predict(image)\n",
    "    \n",
    "    # Apply a sigmoid since our model returns logits\n",
    "    prediction = tf.nn.sigmoid(prediction)\n",
    "    prediction = tf.where(prediction < 0.6, 0, 1)\n",
    "    prediction =  prediction.numpy().ravel()\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIR FallingDown_01\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "DIR FallingDown_02\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "DIR Standing_01\n",
      "DIR Standing_02\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Key Error\n",
      "Dataset Shape: (1625, 300, 300, 3) Labels Shape: (1625,)\n"
     ]
    }
   ],
   "source": [
    "dataset, dataLabels = prepare_data()\n",
    "print('Dataset Shape: {0} Labels Shape: {1}'.format(dataset.shape, dataLabels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = rescaleImage(dataset[0], dataLabels[0])\n",
    "image2 = tf.expand_dims(image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 3.0099s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "model_path=\"./fall_model/model_6.h5\"\n",
    "#load model\n",
    "ResNetmodel = keras.models.load_model(model_path)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Iteration time: %0.4fs\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 4.6981s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "model_path=\"./fall_model/model_5.h5\"\n",
    "#load model\n",
    "EffNetmodel = keras.models.load_model(model_path)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Iteration time: %0.4fs\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration time: 3.4772s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "start_time = time.time()\n",
    "# image , label = getTestimage(dataset, dataLabels)\n",
    "prediction = action_detection_segmentation(image2,ResNetmodel)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Iteration time: %0.4fs\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "start_time = time.time()\n",
    "# image , label = getTestimage(dataset, dataLabels)\n",
    "prediction = action_detection_segmentation(image2,model5)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Iteration time: %0.4fs\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for i in range(30):\n",
    "    start_time = time.time()\n",
    "    one_prediction = action_detection_segmentation(image2, model2)\n",
    "    delta = (time.time() - start_time)\n",
    "    times.append(delta)\n",
    "mean_delta = np.array(times).mean()\n",
    "fps = 1 / mean_delta\n",
    "print('average(sec):{:.2f},fps:{:.2f}'.format(mean_delta, fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for i in range(30):\n",
    "    start_time = time.time()\n",
    "    one_prediction = action_detection_segmentation(image2, model5)\n",
    "    delta = (time.time() - start_time)\n",
    "    times.append(delta)\n",
    "mean_delta = np.array(times).mean()\n",
    "fps = 1 / mean_delta\n",
    "print('average(sec):{:.2f},fps:{:.2f}'.format(mean_delta, fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for 30 runs: 1.23, FPS: 24.29\n"
     ]
    }
   ],
   "source": [
    "execution = timeit.timeit(lambda: action_detection_segmentation(image2, ResNetmodel), number=30)\n",
    "FPS = 30/execution \n",
    "print('Execution time for 30 runs: {:.2f}, FPS: {:.2f}'.format(execution, FPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for 30 runs: 1.16, FPS: 25.78\n"
     ]
    }
   ],
   "source": [
    "execution = timeit.timeit(lambda: action_detection_segmentation(image2, EffNetmodel), number=30)\n",
    "FPS = 30/execution \n",
    "print('Execution time for 30 runs: {:.2f}, FPS: {:.2f}'.format(execution, FPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
